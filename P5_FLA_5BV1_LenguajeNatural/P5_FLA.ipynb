{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flores Lara Alberto\n",
    "5BV1\n",
    "Ingenieria en Inteligencia Artificial\n",
    "Fecha de última modificación: 26/06/2024\n",
    "Descripción detallada de la funcionalidad del programa:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El programa desarrollado tiene como objetivo realizar un análisis de sentimientos sobre un conjunto de reseñas de productos. A continuación se detalla la funcionalidad del programa, los datos requeridos y las funciones implementadas:\n",
    "\n",
    "Datos Requeridos:\n",
    "\n",
    "Un archivo CSV que contenga las reseñas de productos con las siguientes columnas:\n",
    "ProductId: Identificador del producto.\n",
    "UserId: Identificador del usuario.\n",
    "Score: Puntuación dada por el usuario (1 a 5).\n",
    "Summary: Resumen de la reseña.\n",
    "Text: Texto completo de la reseña.\n",
    "Funcionalidades Principales:\n",
    "\n",
    "Cargar y Describir el Conjunto de Datos:\n",
    "Carga el archivo CSV y muestra información general del DataFrame, incluyendo las primeras filas, dimensiones y caracterización de cada columna.\n",
    "\n",
    "Preprocesamiento de Datos:\n",
    "Filtra las columnas relevantes y convierte las puntuaciones en etiquetas de sentimiento (Negativo, Neutral, Positivo).\n",
    "Balancea las clases de sentimiento para asegurar una distribución equitativa.\n",
    "\n",
    "Limpieza de Texto:\n",
    "Convierte el texto a minúsculas, elimina etiquetas HTML, direcciones de correo electrónico, URLs, signos de puntuación y números.\n",
    "Elimina las stopwords y tokeniza el texto.\n",
    "\n",
    "Stemmatización y Lematización:\n",
    "Aplica técnicas de stemming y lematización para reducir las palabras a sus raíces gramaticales.\n",
    "\n",
    "Vectorización:\n",
    "Utiliza la técnica TF-IDF para convertir el texto de las reseñas en vectores numéricos.\n",
    "\n",
    "Análisis de Sentimientos usando Diccionarios:\n",
    "Implementa análisis de sentimientos utilizando los diccionarios Harvard IV-4 y Opinion Lexicon.\n",
    "\n",
    "Análisis de Sentimientos usando Algoritmos de Aprendizaje de Máquina:\n",
    "Entrena y evalúa modelos de regresión logística, árboles de decisión y máquinas de soporte vectorial (SVM) para predecir el sentimiento de las reseñas.\n",
    "\n",
    "Análisis de Sentimientos usando Word Embeddings y Redes Neuronales:\n",
    "Entrena modelos de redes neuronales convolucionales usando embeddings preentrenados (GloVe) y embeddings aprendidos a partir del conjunto de datos.\n",
    "\n",
    "Descripción de las Funciones:\n",
    "\n",
    "load_data(file_path): Carga el conjunto de datos desde un archivo CSV y muestra su información general.\n",
    "preprocess_data(df): Filtra las columnas relevantes, convierte las puntuaciones en etiquetas de sentimiento y balancea las clases.\n",
    "clean_text(text): Realiza la limpieza del texto eliminando elementos no deseados y stopwords.\n",
    "stem_and_lemmatize(text): Aplica técnicas de stemming y lematización al texto.\n",
    "vectorize_text(df): Convierte el texto en vectores numéricos utilizando TF-IDF.\n",
    "analyze_sentiment_harvard(text): Realiza el análisis de sentimiento utilizando el diccionario Harvard IV-4.\n",
    "analyze_sentiment_opinion_lexicon(text): Realiza el análisis de sentimiento utilizando el diccionario Opinion Lexicon.\n",
    "train_ml_models(X_train, y_train): Entrena modelos de regresión logística, árboles de decisión y SVM.\n",
    "evaluate_ml_models(X_test, y_test, models): Evalúa los modelos de aprendizaje de máquina y muestra los resultados.\n",
    "train_nn_model(X_train, y_train, X_test, y_test, embedding_matrix): Entrena una red neuronal convolucional con embeddings preentrenados.\n",
    "train_nn_model_learned(X_train, y_train, X_test, y_test): Entrena una red neuronal convolucional con embeddings aprendidos a partir del conjunto de datos.\n",
    "cross_validate_model(model, X, y, k=5): Realiza k-fold cross-validation y retorna las precisiones del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerias Utilizadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import resample\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "import pickle\n",
    "import pysentiment2 as ps\n",
    "from nltk.corpus import opinion_lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Adquisición de datos\n",
    "2. Análisis Exploratorio de Datos (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informacion general del DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 568454 entries, 0 to 568453\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count   Dtype \n",
      "---  ------                  --------------   ----- \n",
      " 0   Id                      568454 non-null  int64 \n",
      " 1   ProductId               568454 non-null  object\n",
      " 2   UserId                  568454 non-null  object\n",
      " 3   ProfileName             568428 non-null  object\n",
      " 4   HelpfulnessNumerator    568454 non-null  int64 \n",
      " 5   HelpfulnessDenominator  568454 non-null  int64 \n",
      " 6   Score                   568454 non-null  int64 \n",
      " 7   Time                    568454 non-null  int64 \n",
      " 8   Summary                 568427 non-null  object\n",
      " 9   Text                    568454 non-null  object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 43.4+ MB\n",
      "None\n",
      "\n",
      "Primeras filas del DataFrame:\n",
      "   Id   ProductId          UserId                      ProfileName  \\\n",
      "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                     1                       1      5  1303862400   \n",
      "1                     0                       0      1  1346976000   \n",
      "2                     1                       1      4  1219017600   \n",
      "3                     3                       3      2  1307923200   \n",
      "4                     0                       0      5  1350777600   \n",
      "\n",
      "                 Summary                                               Text  \n",
      "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
      "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
      "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
      "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
      "4            Great taffy  Great taffy at a great price.  There was a wid...  \n",
      "\n",
      "Dimensiones del DataFrame:\n",
      "(568454, 10)\n",
      "\n",
      "Caracterizacion de cada columna:\n",
      "\n",
      "Columna: Id\n",
      "Tipo de dato: int64\n",
      "Descripcion:\n",
      "count    568454.000000\n",
      "mean     284227.500000\n",
      "std      164098.679298\n",
      "min           1.000000\n",
      "25%      142114.250000\n",
      "50%      284227.500000\n",
      "75%      426340.750000\n",
      "max      568454.000000\n",
      "Name: Id, dtype: float64\n",
      "\n",
      "Columna: ProductId\n",
      "Tipo de dato: object\n",
      "Descripcion:\n",
      "count         568454\n",
      "unique         74258\n",
      "top       B007JFMH8M\n",
      "freq             913\n",
      "Name: ProductId, dtype: object\n",
      "\n",
      "Columna: UserId\n",
      "Tipo de dato: object\n",
      "Descripcion:\n",
      "count             568454\n",
      "unique            256059\n",
      "top       A3OXHLG6DIBRW8\n",
      "freq                 448\n",
      "Name: UserId, dtype: object\n",
      "\n",
      "Columna: ProfileName\n",
      "Tipo de dato: object\n",
      "Descripcion:\n",
      "count               568428\n",
      "unique              218415\n",
      "top       C. F. Hill \"CFH\"\n",
      "freq                   451\n",
      "Name: ProfileName, dtype: object\n",
      "\n",
      "Columna: HelpfulnessNumerator\n",
      "Tipo de dato: int64\n",
      "Descripcion:\n",
      "count    568454.000000\n",
      "mean          1.743817\n",
      "std           7.636513\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           2.000000\n",
      "max         866.000000\n",
      "Name: HelpfulnessNumerator, dtype: float64\n",
      "\n",
      "Columna: HelpfulnessDenominator\n",
      "Tipo de dato: int64\n",
      "Descripcion:\n",
      "count    568454.00000\n",
      "mean          2.22881\n",
      "std           8.28974\n",
      "min           0.00000\n",
      "25%           0.00000\n",
      "50%           1.00000\n",
      "75%           2.00000\n",
      "max         923.00000\n",
      "Name: HelpfulnessDenominator, dtype: float64\n",
      "\n",
      "Columna: Score\n",
      "Tipo de dato: int64\n",
      "Descripcion:\n",
      "count    568454.000000\n",
      "mean          4.183199\n",
      "std           1.310436\n",
      "min           1.000000\n",
      "25%           4.000000\n",
      "50%           5.000000\n",
      "75%           5.000000\n",
      "max           5.000000\n",
      "Name: Score, dtype: float64\n",
      "\n",
      "Columna: Time\n",
      "Tipo de dato: int64\n",
      "Descripcion:\n",
      "count    5.684540e+05\n",
      "mean     1.296257e+09\n",
      "std      4.804331e+07\n",
      "min      9.393408e+08\n",
      "25%      1.271290e+09\n",
      "50%      1.311120e+09\n",
      "75%      1.332720e+09\n",
      "max      1.351210e+09\n",
      "Name: Time, dtype: float64\n",
      "\n",
      "Columna: Summary\n",
      "Tipo de dato: object\n",
      "Descripcion:\n",
      "count         568427\n",
      "unique        295742\n",
      "top       Delicious!\n",
      "freq            2462\n",
      "Name: Summary, dtype: object\n",
      "\n",
      "Columna: Text\n",
      "Tipo de dato: object\n",
      "Descripcion:\n",
      "count                                                568454\n",
      "unique                                               393579\n",
      "top       This review will make me sound really stupid, ...\n",
      "freq                                                    199\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Carga del conjunto de datos\n",
    "df = pd.read_csv('Reviews.csv')\n",
    "\n",
    "#Información general del DataFrame\n",
    "print(\"Informacion general del DataFrame:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nPrimeras filas del DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDimensiones del DataFrame:\")\n",
    "print(df.shape)\n",
    "\n",
    "print(\"\\nCaracterizacion de cada columna:\")\n",
    "for column in df.columns:\n",
    "    print(f\"\\nColumna: {column}\")\n",
    "    print(f\"Tipo de dato: {df[column].dtype}\")\n",
    "    print(f\"Descripcion:\\n{df[column].describe()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de clases antes del balanceo:\n",
      "Sentiment\n",
      "Positivo    443777\n",
      "Negativo     82037\n",
      "Neutral      42640\n",
      "Name: count, dtype: int64\n",
      "Distribución de clases después del balanceo:\n",
      "Sentiment\n",
      "Negativo    2132\n",
      "Neutral     2132\n",
      "Positivo    2132\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Justificación de las dimensiones necesarias\n",
    "# Mantendremos las columnas relevantes para el análisis de sentimiento: ProductId, UserId, Score, Summary, Text\n",
    "# Eliminaremos las columnas irrelevantes: Id, ProfileName, HelpfulnessNumerator, HelpfulnessDenominator, Time\n",
    "\n",
    "columnas_a_conservar = ['Score', 'Text']\n",
    "df = df[columnas_a_conservar]\n",
    "\n",
    "# Convertir calificaciones en sentimiento\n",
    "def convert_score_to_sentiment(score):\n",
    "    if score in [1, 2]:\n",
    "        return 'Negativo'\n",
    "    elif score == 3:\n",
    "        return 'Neutral'\n",
    "    elif score in [4, 5]:\n",
    "        return 'Positivo'\n",
    "\n",
    "df['Sentiment'] = df['Score'].apply(convert_score_to_sentiment)\n",
    "\n",
    "# Eliminar la columna 'Score' ya que ahora ahora es irrelevante con la nueva columna 'Sentiment'\n",
    "df = df.drop(columns=['Score'])\n",
    "\n",
    "# Observacion deel balance de las clases\n",
    "print(\"Distribución de clases antes del balanceo:\")\n",
    "print(df['Sentiment'].value_counts())\n",
    "\n",
    "# Encontramos la cantidad mínima entre las clases para balancearlas\n",
    "negative = df[df['Sentiment'] == 'Negativo']\n",
    "neutral = df[df['Sentiment'] == 'Neutral']\n",
    "positive = df[df['Sentiment'] == 'Positivo']\n",
    "\n",
    "min_count = min(len(negative), len(neutral), len(positive))\n",
    "min_count = round(min_count/20)\n",
    "\n",
    "negative_balanced = resample(negative, replace=False, n_samples=min_count, random_state=123)\n",
    "neutral_balanced = resample(neutral, replace=False, n_samples=min_count, random_state=123)\n",
    "positive_balanced = resample(positive, replace=False, n_samples=min_count, random_state=123)\n",
    "\n",
    "df_balanced = pd.concat([negative_balanced, neutral_balanced, positive_balanced])\n",
    "df= df_balanced\n",
    "\n",
    "print(\"Distribución de clases después del balanceo:\")\n",
    "print(df_balanced['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Limpieza de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original:\n",
      " 534435    Well, these capsules are sold by Top Line and ...\n",
      "52570     No seriously, this is ridiculous, $45 for a ba...\n",
      "92327     I like strong coffee but was able to use the c...\n",
      "165543    Started out OK, but after a few weeks my dog d...\n",
      "201034    The taste of raspberry is pretty much all I ge...\n",
      "Name: Text, dtype: object\n",
      "\n",
      "Texto limpio:\n",
      " 534435    well capsules sold top line fulfilled amazon t...\n",
      "52570     seriously ridiculous bag corn obligate carnivo...\n",
      "92327     like strong coffee able use cup times reminds ...\n",
      "165543    started ok weeks dog decided didnt like taste ...\n",
      "201034    taste raspberry pretty much get taste smell ch...\n",
      "Name: Cleaned_Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Función para limpiar el texto\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convertir a minúsculas\n",
    "    text = re.sub(r'<.*?>', '', text)  # Eliminar etiquetas HTML\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)  # Eliminar direcciones de correo electrónico\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Eliminar URLs\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Eliminar signos de puntuación\n",
    "    text = re.sub(r'\\d+', '', text)  # Eliminar números\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = ' '.join([word for word in word_tokens if word not in stop_words])  # Eliminar stopwords\n",
    "    return filtered_text\n",
    "\n",
    "# Aplicar la función de limpieza al texto de las reseñas\n",
    "df['Cleaned_Text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# Se muestra una comparacion entre el texto normal y despues de limpiarlo\n",
    "print(\"Texto original:\\n\", df['Text'].head())\n",
    "print(\"\\nTexto limpio:\\n\", df['Cleaned_Text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Transformación y Normalización de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original:\n",
      " 534435    Well, these capsules are sold by Top Line and ...\n",
      "52570     No seriously, this is ridiculous, $45 for a ba...\n",
      "92327     I like strong coffee but was able to use the c...\n",
      "165543    Started out OK, but after a few weeks my dog d...\n",
      "201034    The taste of raspberry is pretty much all I ge...\n",
      "Name: Text, dtype: object\n",
      "\n",
      "Texto limpio:\n",
      " 534435    well capsules sold top line fulfilled amazon t...\n",
      "52570     seriously ridiculous bag corn obligate carnivo...\n",
      "92327     like strong coffee able use cup times reminds ...\n",
      "165543    started ok weeks dog decided didnt like taste ...\n",
      "201034    taste raspberry pretty much get taste smell ch...\n",
      "Name: Cleaned_Text, dtype: object\n",
      "\n",
      "Texto despues de aplicar stemming y lematizacion:\n",
      " 534435    well capsul sold top line fulfil amazon top li...\n",
      "52570     serious ridicul bag corn oblig carnivor brewer...\n",
      "92327     like strong coffe abl use cup time remind star...\n",
      "165543    start ok week dog decid didnt like tast much b...\n",
      "201034    tast raspberri pretti much get tast smell choc...\n",
      "Name: Stemmed_Lemmatized_Text, dtype: object\n",
      "\n",
      "Texto tokenizado:\n",
      " 534435    [well, capsul, sold, top, line, fulfil, amazon...\n",
      "52570     [serious, ridicul, bag, corn, oblig, carnivor,...\n",
      "92327     [like, strong, coffe, abl, use, cup, time, rem...\n",
      "165543    [start, ok, week, dog, decid, didnt, like, tas...\n",
      "201034    [tast, raspberri, pretti, much, get, tast, sme...\n",
      "Name: Tokenized_Text, dtype: object\n",
      "\n",
      "Muestra de Matriz TF-IDF):\n",
      "   (0, 7425)\t0.16430213157685694\n",
      "  (0, 11634)\t0.08413966522039766\n",
      "  (0, 14115)\t0.10557533735563944\n",
      "  (0, 2946)\t0.0801688769525467\n",
      "  (0, 3724)\t0.2389351734935208\n",
      "  (0, 16922)\t0.11071366012468895\n",
      "  (0, 9874)\t0.21479806968406706\n",
      "  (0, 4140)\t0.16508852397994261\n",
      "  (0, 2034)\t0.07305552845570645\n",
      "  (0, 11285)\t0.12485417652463518\n",
      "  (0, 905)\t0.1876756402059262\n",
      "  (0, 106)\t0.19931448215553627\n",
      "  (0, 2205)\t0.2389351734935208\n",
      "  (0, 9671)\t0.2206761593286767\n",
      "  (0, 16588)\t0.08820648990600786\n",
      "  (0, 11735)\t0.06317921938134158\n",
      "  (0, 12441)\t0.2059346543707703\n",
      "  (0, 13245)\t0.16068795367726066\n",
      "  (0, 455)\t0.08562144785732353\n",
      "  (0, 5970)\t0.2059346543707703\n",
      "  (0, 8536)\t0.2644045757057443\n",
      "  (0, 15537)\t0.2450664764302352\n",
      "  (0, 13786)\t0.1407922523799617\n",
      "  (0, 2214)\t0.5630269206177786\n",
      "  (0, 16667)\t0.08613449547426245\n",
      "  :\t:\n",
      "  (3, 8499)\t0.0628364133458906\n",
      "  (3, 9675)\t0.08976993480123127\n",
      "  (3, 16667)\t0.10335811528317176\n",
      "  (4, 8367)\t0.16693569948583786\n",
      "  (4, 3105)\t0.18654764600261817\n",
      "  (4, 3989)\t0.25209438008092727\n",
      "  (4, 5165)\t0.17475173166313554\n",
      "  (4, 6788)\t0.20279526521910138\n",
      "  (4, 8698)\t0.10056554148305574\n",
      "  (4, 9895)\t0.1420761897002991\n",
      "  (4, 11694)\t0.15657536392039884\n",
      "  (4, 7329)\t0.1614807912574148\n",
      "  (4, 1726)\t0.1271297095351127\n",
      "  (4, 12746)\t0.24229724004487968\n",
      "  (4, 10724)\t0.23552432529565942\n",
      "  (4, 2689)\t0.29152142504543554\n",
      "  (4, 13637)\t0.15028185034184613\n",
      "  (4, 6138)\t0.09892620788267228\n",
      "  (4, 11620)\t0.15084397558123375\n",
      "  (4, 12098)\t0.46900692741191213\n",
      "  (4, 5438)\t0.1854133395775647\n",
      "  (4, 5509)\t0.09125778995512411\n",
      "  (4, 14826)\t0.23266242839614087\n",
      "  (4, 9675)\t0.10871362353382694\n",
      "  (4, 2946)\t0.3495003246146411\n"
     ]
    }
   ],
   "source": [
    "# a. Convertir todos los términos a minúsculas (ya se hizo en la limpieza)\n",
    "\n",
    "# b. Aplicar stemming y lematizacion al texto\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def stem_and_lemmatize(text):\n",
    "    words = word_tokenize(text)\n",
    "    stemmed = [stemmer.stem(word) for word in words]\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in stemmed]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "df['Stemmed_Lemmatized_Text'] = df['Cleaned_Text'].apply(stem_and_lemmatize)\n",
    "\n",
    "# c. Tokenizar las reseñas\n",
    "df['Tokenized_Text'] = df['Stemmed_Lemmatized_Text'].apply(word_tokenize)\n",
    "\n",
    "# d. Vectorizar las reseñas usando TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Stemmed_Lemmatized_Text'])\n",
    "\n",
    "print(\"Texto original:\\n\", df['Text'].head())\n",
    "print(\"\\nTexto limpio:\\n\", df['Cleaned_Text'].head())\n",
    "print(\"\\nTexto despues de aplicar stemming y lematizacion:\\n\", df['Stemmed_Lemmatized_Text'].head())\n",
    "print(\"\\nTexto tokenizado:\\n\", df['Tokenized_Text'].head())\n",
    "print(\"\\nMuestra de Matriz TF-IDF):\\n\", tfidf_matrix[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Análisis de Sentimientos con Diccionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original:\n",
      " 534435    Well, these capsules are sold by Top Line and ...\n",
      "52570     No seriously, this is ridiculous, $45 for a ba...\n",
      "92327     I like strong coffee but was able to use the c...\n",
      "165543    Started out OK, but after a few weeks my dog d...\n",
      "201034    The taste of raspberry is pretty much all I ge...\n",
      "Name: Text, dtype: object\n",
      "\n",
      "Sentimiento Harvard IV-4:\n",
      " 534435    0.714286\n",
      "52570     0.250000\n",
      "92327     0.500000\n",
      "165543    0.142857\n",
      "201034   -0.090909\n",
      "Name: Harvard_Sentiment, dtype: float64\n",
      "\n",
      "Sentimiento Opinion Lexicon:\n",
      " 534435    3\n",
      "52570     1\n",
      "92327     5\n",
      "165543    0\n",
      "201034   -4\n",
      "Name: Opinion_Lexicon_Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# a. Análisis de sentimientos usando Harvard IV-4 con pysentiment2\n",
    "harvard = ps.HIV4()\n",
    "\n",
    "def analyze_sentiment_harvard(text):\n",
    "    tokens = harvard.tokenize(text)\n",
    "    score = harvard.get_score(tokens)\n",
    "    return score\n",
    "\n",
    "df['Harvard_Sentiment'] = df['Stemmed_Lemmatized_Text'].apply(lambda x: analyze_sentiment_harvard(x)['Polarity'])\n",
    "\n",
    "# b. Análisis de sentimientos usando Opinion Lexicon con NLTK\n",
    "positive_words = opinion_lexicon.positive()\n",
    "negative_words = opinion_lexicon.negative()\n",
    "\n",
    "def analyze_sentiment_opinion_lexicon(text):    \n",
    "    tokens = word_tokenize(text)\n",
    "    pos_score = sum(1 for word in tokens if word in positive_words)\n",
    "    neg_score = sum(1 for word in tokens if word in negative_words)\n",
    "    return pos_score - neg_score\n",
    "\n",
    "df['Opinion_Lexicon_Sentiment'] = df['Stemmed_Lemmatized_Text'].apply(analyze_sentiment_opinion_lexicon)\n",
    "\n",
    "# Mostramos algunas reseñas con sus análisis de sentimientos\n",
    "print(\"Texto original:\\n\", df['Text'].head())\n",
    "print(\"\\nSentimiento Harvard IV-4:\\n\", df['Harvard_Sentiment'].head())\n",
    "print(\"\\nSentimiento Opinion Lexicon:\\n\", df['Opinion_Lexicon_Sentiment'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Análisis de Sentimientos con Algoritmos de Aprendizaje de Máquina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regresión Logística\n",
      "Accuracy: 0.66796875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.65      0.68       446\n",
      "           1       0.56      0.63      0.59       404\n",
      "           2       0.76      0.72      0.74       430\n",
      "\n",
      "    accuracy                           0.67      1280\n",
      "   macro avg       0.67      0.67      0.67      1280\n",
      "weighted avg       0.67      0.67      0.67      1280\n",
      "\n",
      "Árboles de Decisión\n",
      "Accuracy: 0.51171875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.48      0.51       446\n",
      "           1       0.45      0.50      0.47       404\n",
      "           2       0.54      0.56      0.55       430\n",
      "\n",
      "    accuracy                           0.51      1280\n",
      "   macro avg       0.51      0.51      0.51      1280\n",
      "weighted avg       0.51      0.51      0.51      1280\n",
      "\n",
      "Máquinas de Soporte Vectorial\n",
      "Accuracy: 0.66484375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.66      0.68       446\n",
      "           1       0.55      0.65      0.60       404\n",
      "           2       0.77      0.68      0.73       430\n",
      "\n",
      "    accuracy                           0.66      1280\n",
      "   macro avg       0.67      0.66      0.67      1280\n",
      "weighted avg       0.68      0.66      0.67      1280\n",
      "\n",
      "Precisión usando Regresión Logística: [0.66796875 0.64190774 0.65285379 0.65910868 0.64738077]\n",
      "Precisión Media de Validación usando Regresión Logística: 0.6538439454652072\n",
      "Precisión de Validación usando Árboles de Decisión: [0.50078125 0.50351837 0.52150117 0.48631744 0.51133698]\n",
      "Precisión Media de Validación usando Árboles de Decisión: 0.5046910428068804\n",
      "Precisión de Validación usando SVM: [0.66484375 0.64816263 0.66379984 0.6645817  0.6645817 ]\n",
      "Precisión Media de Validación usando SVM: 0.6611939259186865\n"
     ]
    }
   ],
   "source": [
    "# Convertir la columna de sentimiento a un valor numérico: Negativo=0, Neutral=1, Positivo=2\n",
    "sentiment_mapping = {'Negativo': 0, 'Neutral': 1, 'Positivo': 2}\n",
    "df['Sentiment_Num'] = df['Sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# Vectorizamos las reseñas usando TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(df['Stemmed_Lemmatized_Text'])\n",
    "y = df['Sentiment_Num']\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# c. Regresión Logística\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "print(\"Regresión Logística\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_logreg))\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "\n",
    "# d. Árboles de Decisión\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "print(\"Árboles de Decisión\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "\n",
    "# e. Máquinas de Soporte Vectorial (SVM)\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "print(\"Máquinas de Soporte Vectorial\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# Función para realizar la validación cruzada y obtener las accuracys\n",
    "def cross_validate_model(model, X, y, k=5):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    accuracies = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
    "    return accuracies\n",
    "\n",
    "# Realizar validación cruzada con k=5\n",
    "logreg_accuracies = cross_validate_model(LogisticRegression(max_iter=1000), X, y, k=5)\n",
    "dt_accuracies = cross_validate_model(DecisionTreeClassifier(), X, y, k=5)\n",
    "svm_accuracies = cross_validate_model(SVC(), X, y, k=5)\n",
    "\n",
    "print(\"Precisión usando Regresión Logística:\", logreg_accuracies)\n",
    "print(\"Precisión Media de Validación usando Regresión Logística:\", np.mean(logreg_accuracies))\n",
    "\n",
    "print(\"Precisión de Validación usando Árboles de Decisión:\", dt_accuracies)\n",
    "print(\"Precisión Media de Validación usando Árboles de Decisión:\", np.mean(dt_accuracies))\n",
    "\n",
    "print(\"Precisión de Validación usando SVM:\", svm_accuracies)\n",
    "print(\"Precisión Media de Validación usando SVM:\", np.mean(svm_accuracies))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Análisis de Sentimientos con Word Embeddings y Redes Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\albsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">865,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │       \u001b[38;5;34m865,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">865,400</span> (3.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m865,400\u001b[0m (3.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">865,400</span> (3.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m865,400\u001b[0m (3.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3288 - loss: 1.2802 - val_accuracy: 0.4039 - val_loss: 1.0903\n",
      "Epoch 2/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4152 - loss: 1.0713 - val_accuracy: 0.4523 - val_loss: 1.0573\n",
      "Epoch 3/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4651 - loss: 1.0350 - val_accuracy: 0.4914 - val_loss: 1.0223\n",
      "Epoch 4/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4983 - loss: 0.9819 - val_accuracy: 0.4820 - val_loss: 1.0058\n",
      "Epoch 5/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5548 - loss: 0.9164 - val_accuracy: 0.5367 - val_loss: 0.9737\n",
      "Epoch 6/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6077 - loss: 0.8686 - val_accuracy: 0.5250 - val_loss: 0.9678\n",
      "Epoch 7/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6311 - loss: 0.8120 - val_accuracy: 0.5492 - val_loss: 0.9569\n",
      "Epoch 8/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6672 - loss: 0.7526 - val_accuracy: 0.5398 - val_loss: 0.9883\n",
      "Epoch 9/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6759 - loss: 0.7154 - val_accuracy: 0.5383 - val_loss: 0.9847\n",
      "Epoch 10/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7293 - loss: 0.6423 - val_accuracy: 0.5430 - val_loss: 1.0047\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Accuracy with pre-trained embeddings: 0.54296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\albsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_1          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_1          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3488 - loss: 1.0951 - val_accuracy: 0.5180 - val_loss: 1.0204\n",
      "Epoch 2/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5574 - loss: 0.9497 - val_accuracy: 0.6125 - val_loss: 0.8539\n",
      "Epoch 3/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7328 - loss: 0.6676 - val_accuracy: 0.6453 - val_loss: 0.8197\n",
      "Epoch 4/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8831 - loss: 0.3951 - val_accuracy: 0.6391 - val_loss: 0.9119\n",
      "Epoch 5/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9523 - loss: 0.1932 - val_accuracy: 0.6219 - val_loss: 1.0611\n",
      "Epoch 6/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9848 - loss: 0.0931 - val_accuracy: 0.6148 - val_loss: 1.2328\n",
      "Epoch 7/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9901 - loss: 0.0566 - val_accuracy: 0.6125 - val_loss: 1.3946\n",
      "Epoch 8/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9882 - loss: 0.0438 - val_accuracy: 0.6281 - val_loss: 1.4896\n",
      "Epoch 9/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9932 - loss: 0.0326 - val_accuracy: 0.6266 - val_loss: 1.6119\n",
      "Epoch 10/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9925 - loss: 0.0299 - val_accuracy: 0.6203 - val_loss: 1.7371\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Accuracy with learned embeddings: 0.6203125\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987us/step\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Accuracies using pre-trained embeddings: [0.5234375, 0.8131352619233776, 0.8569194683346364, 0.9241594996090696, 0.9538702111024238]\n",
      "Mean accuracy using pre-trained embeddings: 0.8143043881939015\n",
      "Accuracies using learned embeddings: [0.59609375, 0.980453479280688, 0.9937451133698202, 0.9976544175136826, 0.9960906958561376]\n",
      "Mean accuracy using learned embeddings: 0.9128074912040656\n"
     ]
    }
   ],
   "source": [
    "# Tokenización y Padding\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df['Stemmed_Lemmatized_Text'])\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(df['Stemmed_Lemmatized_Text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=100, truncating='post')\n",
    "\n",
    "# One-hot encoding de las etiquetas\n",
    "sentiment_labels = pd.get_dummies(df['Sentiment_Num']).values\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_sequences, sentiment_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Cargar los embeddings preentrenados de GloVe (50 dimensiones)\n",
    "embedding_index = {}\n",
    "with open('glove.6B.50d.txt', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "embedding_dim = 50\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Crear el modelo de red neuronal con embeddings preentrenados\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=100, trainable=False))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "y_pred = np.argmax(model.predict(x_test), axis=-1)\n",
    "y_test_labels = np.argmax(y_test, axis=-1)\n",
    "print(\"Precisión con embeddings preentrenados:\", accuracy_score(y_test_labels, y_pred))\n",
    "\n",
    "# Crear el modelo usando una capa de embeddings aprendida a partir del cuerpo de documentos\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(5000, 100, input_length=100))\n",
    "model2.add(Conv1D(64, 5, activation='relu'))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(32, activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(3, activation='softmax'))\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model2.summary()\n",
    "\n",
    "# Entrenar el modelo\n",
    "model2.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "y_pred2 = np.argmax(model2.predict(x_test), axis=-1)\n",
    "print(\"Accuracy with learned embeddings:\", accuracy_score(y_test_labels, y_pred2))\n",
    "\n",
    "# Crear una función para entrenar y evaluar el modelo con k-fold cross-validation\n",
    "def cross_validate_model_nn(model, X, y, k=5):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "        y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "        y_test_labels = np.argmax(y_test, axis=-1)\n",
    "        accuracies.append(accuracy_score(y_test_labels, y_pred))\n",
    "    return accuracies\n",
    "\n",
    "# Evaluar ambos modelos usando k-fold cross-validation\n",
    "accuracies_pretrained = cross_validate_model_nn(model, np.array(padded_sequences), np.array(sentiment_labels), k=5)\n",
    "accuracies_learned = cross_validate_model_nn(model2, np.array(padded_sequences), np.array(sentiment_labels), k=5)\n",
    "\n",
    "print(\"Precisión utilizando embeddings preentrenados:\", accuracies_pretrained)\n",
    "print(\"Precisión media utilizando embeddings preentrenados:\", np.mean(accuracies_pretrained))\n",
    "print(\"Precisión utilizando embeddings aprendidos:\", accuracies_learned)\n",
    "print(\"Precisión media utilizando embeddings aprendidos:\", np.mean(accuracies_learned))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
