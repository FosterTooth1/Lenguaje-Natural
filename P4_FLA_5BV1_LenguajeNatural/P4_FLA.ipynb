{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flores Lara Alberto\n",
    "5BV1\n",
    "Ingenieria en Inteligencia Artificial\n",
    "10/06/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librerías necesarias\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "# Descarga de los recursos necesarios\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Definir las funciones del programa\n",
    "\n",
    "# Convierte todo el texto a minúsculas\n",
    "def minusculas(texto):\n",
    "    return texto.lower()\n",
    "\n",
    "#Elimina todos los caracteres especiales del texto\n",
    "def caracteresesp(texto):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', texto)\n",
    "\n",
    "# Quita los espacios extra del texto\n",
    "def quitaresp(texto):\n",
    "    return \" \".join(texto.split())\n",
    "\n",
    "#Elimina las palabras comunes stopwords\n",
    "def stooopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [w for w in tokens if w.lower() not in stop_words]\n",
    "\n",
    "#Tokeniza el texto\n",
    "def tokenizar(texto):\n",
    "    return word_tokenize(texto)\n",
    "\n",
    "def aplicar_procesamiento(cuerpos):\n",
    "    cuerpos_procesados = []\n",
    "    for cuerpo in cuerpos:\n",
    "        cuerpo_procesado = minusculas(cuerpo)\n",
    "        cuerpo_procesado = caracteresesp(cuerpo_procesado)\n",
    "        cuerpo_procesado = quitaresp(cuerpo_procesado)\n",
    "        tokens = tokenizar(cuerpo_procesado)\n",
    "        tokens = stooopwords(tokens)\n",
    "        cuerpos_procesados.append(tokens)\n",
    "    return cuerpos_procesados\n",
    "\n",
    "# Limpiamos todos los caracteres que no esten en formato utf-8\n",
    "def clean_text(text):\n",
    "    return text.encode('utf-8', 'ignore').decode('utf-8')\n",
    "\n",
    "# Función para etiquetar gramaticalmente el texto\n",
    "def pos_tag_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "    return tagged_sentences\n",
    "\n",
    "# Función para encontrar la palabra más frecuente de una categoría gramatical dada\n",
    "def most_frequent_word(tagged_text, pos_tag_prefix):\n",
    "    words = [word for word, tag in tagged_text if tag.startswith(pos_tag_prefix)]\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    return fdist.max()\n",
    "\n",
    "# Función para calcular la similitud entre palabras usando synsets\n",
    "def word_similarity(word, pos_tag, metric='wup_similarity'):\n",
    "    synsets = wn.synsets(word, pos=pos_tag)\n",
    "    if not synsets:\n",
    "        return []\n",
    "    \n",
    "    similarities = []\n",
    "    for synset in synsets:\n",
    "        for other_synset in synsets:\n",
    "            if synset != other_synset:\n",
    "                if metric == 'wup_similarity':\n",
    "                    similarity = synset.wup_similarity(other_synset)\n",
    "                elif metric == 'path_similarity':\n",
    "                    similarity = synset.path_similarity(other_synset)\n",
    "                else:\n",
    "                    similarity = 0\n",
    "                if similarity is not None:\n",
    "                    similarities.append((other_synset, similarity))\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:5]\n",
    "\n",
    "# Función para convertir etiquetas a las utilizadas por WordNet\n",
    "def convert_tag(tag):\n",
    "    tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
    "    try:\n",
    "        return tag_dict[tag[0]]\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "# Función para convertir documento a synsets\n",
    "def doc_to_synsets(doc):\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    wntag = [convert_tag(tag) for tag in [tag[1] for tag in pos]]\n",
    "    synsets = [wn.synsets(token, tag)[0] for token, tag in zip(tokens, wntag) if wn.synsets(token, tag)]\n",
    "    return synsets\n",
    "\n",
    "# Función para calcular la puntuación de similitud\n",
    "def similarity_score(s1, s2):\n",
    "    scores = []\n",
    "    for synset in s1:\n",
    "        best_score = max([synset.path_similarity(ss) for ss in s2 if synset.path_similarity(ss)] or [0])\n",
    "        if best_score:\n",
    "            scores.append(best_score)\n",
    "    return sum(scores) / len(scores) if scores else 0\n",
    "\n",
    "# Función para calcular la similitud entre dos documentos\n",
    "def document_path_similarity(doc1, doc2):\n",
    "    synsets1 = doc_to_synsets(doc1)\n",
    "    synsets2 = doc_to_synsets(doc2)\n",
    "    return (similarity_score(synsets1, synsets2) + similarity_score(synsets2, synsets1)) / 2\n",
    "\n",
    "# Función para cargar las incrustaciones de GloVe\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings_dict = {}\n",
    "    with open(glove_file, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vectors = np.asarray(values[1:], 'float32')\n",
    "            embeddings_dict[word] = vectors\n",
    "    return embeddings_dict\n",
    "\n",
    "# Función para encontrar las palabras más similares usando GloVe\n",
    "def find_closest_embeddings(embedding, embeddings_dict, top_n=5):\n",
    "    return sorted(embeddings_dict.keys(), key=lambda word: spatial.distance.cosine(embeddings_dict[word], embedding))[:top_n]\n",
    "\n",
    "# Función para obtener las top palabras representativas de un texto usando BERT\n",
    "def obtener_top_palabras(texto, model, tokenizer, top_n=5):\n",
    "    # Tokenizamos el texto\n",
    "    tokens = tokenizer(texto, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    # Pasamos el input por BERT para obtener los embeddings\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    word_embeddings = output.last_hidden_state.squeeze(0)\n",
    "    \n",
    "    # Usamos la media de las representaciones para obtener el embedding de la oración\n",
    "    sentence_embedding = word_embeddings.mean(dim=0)\n",
    "\n",
    "    # Se calcula la similitud entre las palabras y la oración\n",
    "    similarities = torch.nn.functional.cosine_similarity(word_embeddings, sentence_embedding.unsqueeze(0), dim=1)\n",
    "\n",
    "    # Ordenamos por similitud y obtener los índices de las palabras más similares\n",
    "    sorted_indices = torch.argsort(similarities, descending=True).tolist()\n",
    "    \n",
    "    # Convertir tokens a palabras originales\n",
    "    tokens_text = tokenizer.convert_ids_to_tokens(tokens['input_ids'].squeeze(0).tolist())\n",
    "\n",
    "    # Obtenemos las top_n palabras más representativas\n",
    "    top_palabras = [tokens_text[idx] for idx in sorted_indices if tokens_text[idx] not in [\"[CLS]\", \"[SEP]\"]][:top_n]\n",
    "\n",
    "    return top_palabras\n",
    "\n",
    "# Función para calcular la similitud entre documentos usando BERT\n",
    "def bert_similarity(doc1, doc2, model, tokenizer):\n",
    "    tokens1 = tokenizer(doc1, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    tokens2 = tokenizer(doc2, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output1 = model(**tokens1)\n",
    "        output2 = model(**tokens2)\n",
    "    \n",
    "    embedding1 = output1.last_hidden_state.mean(dim=1).squeeze()\n",
    "    embedding2 = output2.last_hidden_state.mean(dim=1).squeeze()\n",
    "    \n",
    "    cosine_similarity = torch.nn.functional.cosine_similarity(embedding1, embedding2, dim=0)\n",
    "    return cosine_similarity.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generación de cuerpo de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías utilizadas: nltk, PlaintextCorpusReader\n",
    "# Descripción: Lee y organiza documentos de texto para su posterior análisis\n",
    "\n",
    "# Definimos el lugar donde se encuentran los archivos de texto\n",
    "corpus_root = ''\n",
    "corpus_files = [f'Libro_{i}.txt' for i in range(1, 11)]\n",
    "\n",
    "# Creamos el corpus con la codificación utf8\n",
    "corpus = PlaintextCorpusReader(corpus_root, corpus_files, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Preparación de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías utilizadas: nltk, re, stopwords\n",
    "# Funciones utilizadas: clean_text, aplicar_procesamiento, pos_tag_text\n",
    "# Descripción: Limpia, preprocesa y etiqueta gramaticalmente el texto de los documentos\n",
    "\n",
    "# Procesar y preprocesar cada documento en el corpus\n",
    "tagged_docs = {}\n",
    "for fileid in corpus.fileids():\n",
    "    raw_text = corpus.raw(fileid)\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    \n",
    "    # Aplicar preprocesamiento\n",
    "    cuerpos_procesados = aplicar_procesamiento([cleaned_text])\n",
    "    cuerpos_procesados = ' '.join(cuerpos_procesados[0])\n",
    "    \n",
    "    tagged_docs[fileid] = pos_tag_text(cuerpos_procesados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Similitud de palabras con synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento: Libro_1.txt\n",
      "Verbo más frecuente: said\n",
      "Similitudes WUP (verbo): [(Synset('say.v.11'), 0.5333333333333333), (Synset('say.v.07'), 0.5333333333333333), (Synset('say.v.09'), 0.5), (Synset('pronounce.v.01'), 0.5), (Synset('pronounce.v.01'), 0.4)]\n",
      "Similitudes Path (verbo): [(Synset('say.v.08'), 0.3333333333333333), (Synset('say.v.09'), 0.3333333333333333), (Synset('state.v.01'), 0.3333333333333333), (Synset('pronounce.v.01'), 0.3333333333333333), (Synset('pronounce.v.01'), 0.25)]\n",
      "Sustantivo más frecuente: rnine\n",
      "Similitudes WUP (sustantivo): []\n",
      "Similitudes Path (sustantivo): []\n",
      "\n",
      "\n",
      "Documento: Libro_2.txt\n",
      "Verbo más frecuente: said\n",
      "Similitudes WUP (verbo): [(Synset('say.v.11'), 0.5333333333333333), (Synset('say.v.07'), 0.5333333333333333), (Synset('say.v.09'), 0.5), (Synset('pronounce.v.01'), 0.5), (Synset('pronounce.v.01'), 0.4)]\n",
      "Similitudes Path (verbo): [(Synset('say.v.08'), 0.3333333333333333), (Synset('say.v.09'), 0.3333333333333333), (Synset('state.v.01'), 0.3333333333333333), (Synset('pronounce.v.01'), 0.3333333333333333), (Synset('pronounce.v.01'), 0.25)]\n",
      "Sustantivo más frecuente: door\n",
      "Similitudes WUP (sustantivo): [(Synset('door.n.05'), 0.75), (Synset('door.n.04'), 0.75), (Synset('door.n.04'), 0.7058823529411765), (Synset('door.n.01'), 0.7058823529411765), (Synset('door.n.05'), 0.631578947368421)]\n",
      "Similitudes Path (sustantivo): [(Synset('door.n.05'), 0.2), (Synset('door.n.04'), 0.2), (Synset('door.n.04'), 0.16666666666666666), (Synset('door.n.01'), 0.16666666666666666), (Synset('door.n.04'), 0.14285714285714285)]\n",
      "\n",
      "\n",
      "Documento: Libro_3.txt\n",
      "Verbo más frecuente: came\n",
      "Similitudes WUP (verbo): [(Synset('come.v.16'), 0.6666666666666666), (Synset('issue_forth.v.01'), 0.5), (Synset('derive.v.05'), 0.5), (Synset('arrive.v.01'), 0.5), (Synset('derive.v.05'), 0.5)]\n",
      "Similitudes Path (verbo): [(Synset('come.v.16'), 0.5), (Synset('arrive.v.01'), 0.5), (Synset('come.v.11'), 0.3333333333333333), (Synset('issue_forth.v.01'), 0.3333333333333333), (Synset('derive.v.05'), 0.3333333333333333)]\n",
      "Sustantivo más frecuente: man\n",
      "Similitudes WUP (sustantivo): [(Synset('world.n.08'), 0.9655172413793104), (Synset('homo.n.02'), 0.9655172413793104), (Synset('man.n.03'), 0.7058823529411765), (Synset('man.n.01'), 0.7058823529411765), (Synset('man.n.06'), 0.7058823529411765)]\n",
      "Similitudes Path (sustantivo): [(Synset('world.n.08'), 0.5), (Synset('homo.n.02'), 0.5), (Synset('man.n.06'), 0.3333333333333333), (Synset('man.n.08'), 0.3333333333333333), (Synset('man.n.01'), 0.3333333333333333)]\n",
      "\n",
      "\n",
      "Documento: Libro_4.txt\n",
      "Verbo más frecuente: said\n",
      "Similitudes WUP (verbo): [(Synset('say.v.11'), 0.5333333333333333), (Synset('say.v.07'), 0.5333333333333333), (Synset('say.v.09'), 0.5), (Synset('pronounce.v.01'), 0.5), (Synset('pronounce.v.01'), 0.4)]\n",
      "Similitudes Path (verbo): [(Synset('say.v.08'), 0.3333333333333333), (Synset('say.v.09'), 0.3333333333333333), (Synset('state.v.01'), 0.3333333333333333), (Synset('pronounce.v.01'), 0.3333333333333333), (Synset('pronounce.v.01'), 0.25)]\n",
      "Sustantivo más frecuente: lupin\n",
      "Similitudes WUP (sustantivo): []\n",
      "Similitudes Path (sustantivo): []\n",
      "\n",
      "\n",
      "Documento: Libro_5.txt\n",
      "Verbo más frecuente: said\n",
      "Similitudes WUP (verbo): [(Synset('say.v.11'), 0.5333333333333333), (Synset('say.v.07'), 0.5333333333333333), (Synset('say.v.09'), 0.5), (Synset('pronounce.v.01'), 0.5), (Synset('pronounce.v.01'), 0.4)]\n",
      "Similitudes Path (verbo): [(Synset('say.v.08'), 0.3333333333333333), (Synset('say.v.09'), 0.3333333333333333), (Synset('state.v.01'), 0.3333333333333333), (Synset('pronounce.v.01'), 0.3333333333333333), (Synset('pronounce.v.01'), 0.25)]\n",
      "Sustantivo más frecuente: man\n",
      "Similitudes WUP (sustantivo): [(Synset('world.n.08'), 0.9655172413793104), (Synset('homo.n.02'), 0.9655172413793104), (Synset('man.n.03'), 0.7058823529411765), (Synset('man.n.01'), 0.7058823529411765), (Synset('man.n.06'), 0.7058823529411765)]\n",
      "Similitudes Path (sustantivo): [(Synset('world.n.08'), 0.5), (Synset('homo.n.02'), 0.5), (Synset('man.n.06'), 0.3333333333333333), (Synset('man.n.08'), 0.3333333333333333), (Synset('man.n.01'), 0.3333333333333333)]\n",
      "\n",
      "\n",
      "Documento: Libro_6.txt\n",
      "Verbo más frecuente: said\n",
      "Similitudes WUP (verbo): [(Synset('say.v.11'), 0.5333333333333333), (Synset('say.v.07'), 0.5333333333333333), (Synset('say.v.09'), 0.5), (Synset('pronounce.v.01'), 0.5), (Synset('pronounce.v.01'), 0.4)]\n",
      "Similitudes Path (verbo): [(Synset('say.v.08'), 0.3333333333333333), (Synset('say.v.09'), 0.3333333333333333), (Synset('state.v.01'), 0.3333333333333333), (Synset('pronounce.v.01'), 0.3333333333333333), (Synset('pronounce.v.01'), 0.25)]\n",
      "Sustantivo más frecuente: irvin\n",
      "Similitudes WUP (sustantivo): []\n",
      "Similitudes Path (sustantivo): []\n",
      "\n",
      "\n",
      "Documento: Libro_7.txt\n",
      "Verbo más frecuente: said\n",
      "Similitudes WUP (verbo): [(Synset('say.v.11'), 0.5333333333333333), (Synset('say.v.07'), 0.5333333333333333), (Synset('say.v.09'), 0.5), (Synset('pronounce.v.01'), 0.5), (Synset('pronounce.v.01'), 0.4)]\n",
      "Similitudes Path (verbo): [(Synset('say.v.08'), 0.3333333333333333), (Synset('say.v.09'), 0.3333333333333333), (Synset('state.v.01'), 0.3333333333333333), (Synset('pronounce.v.01'), 0.3333333333333333), (Synset('pronounce.v.01'), 0.25)]\n",
      "Sustantivo más frecuente: raffles\n",
      "Similitudes WUP (sustantivo): [(Synset('raffle.n.01'), 0.1111111111111111), (Synset('raffles.n.01'), 0.1111111111111111)]\n",
      "Similitudes Path (sustantivo): [(Synset('raffle.n.01'), 0.058823529411764705), (Synset('raffles.n.01'), 0.058823529411764705)]\n",
      "\n",
      "\n",
      "Documento: Libro_8.txt\n",
      "Verbo más frecuente: fleming\n",
      "Similitudes WUP (verbo): []\n",
      "Similitudes Path (verbo): []\n",
      "Sustantivo más frecuente: rand\n",
      "Similitudes WUP (sustantivo): [(Synset('witwatersrand.n.01'), 0.4), (Synset('rand.n.02'), 0.4), (Synset('witwatersrand.n.01'), 0.14285714285714285), (Synset('rand.n.01'), 0.14285714285714285), (Synset('rand.n.02'), 0.13333333333333333)]\n",
      "Similitudes Path (sustantivo): [(Synset('witwatersrand.n.01'), 0.1), (Synset('rand.n.02'), 0.1), (Synset('witwatersrand.n.01'), 0.07692307692307693), (Synset('rand.n.01'), 0.07692307692307693), (Synset('rand.n.02'), 0.07142857142857142)]\n",
      "\n",
      "\n",
      "Documento: Libro_9.txt\n",
      "Verbo más frecuente: said\n",
      "Similitudes WUP (verbo): [(Synset('say.v.11'), 0.5333333333333333), (Synset('say.v.07'), 0.5333333333333333), (Synset('say.v.09'), 0.5), (Synset('pronounce.v.01'), 0.5), (Synset('pronounce.v.01'), 0.4)]\n",
      "Similitudes Path (verbo): [(Synset('say.v.08'), 0.3333333333333333), (Synset('say.v.09'), 0.3333333333333333), (Synset('state.v.01'), 0.3333333333333333), (Synset('pronounce.v.01'), 0.3333333333333333), (Synset('pronounce.v.01'), 0.25)]\n",
      "Sustantivo más frecuente: lupin\n",
      "Similitudes WUP (sustantivo): []\n",
      "Similitudes Path (sustantivo): []\n",
      "\n",
      "\n",
      "Documento: Libro_10.txt\n",
      "Verbo más frecuente: captain\n",
      "Similitudes WUP (verbo): []\n",
      "Similitudes Path (verbo): []\n",
      "Sustantivo más frecuente: denison\n",
      "Similitudes WUP (sustantivo): []\n",
      "Similitudes Path (sustantivo): []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Librerías utilizadas: nltk, wordnet\n",
    "# Funciones utilizadas: most_frequent_word, word_similarity\n",
    "# Descripción: Encuentra la palabra más frecuente en cada categoría gramatical y calcula sus similitudes con otras palabras usando synsets\n",
    "\n",
    "# Encontrar las palabras más frecuentes y calcular sus similitudes\n",
    "for fileid, tagged_text in tagged_docs.items():\n",
    "    flat_tagged_text = [item for sublist in tagged_text for item in sublist]\n",
    "    \n",
    "    # Verbo más frecuente\n",
    "    most_freq_verb = most_frequent_word(flat_tagged_text, 'VB')\n",
    "    verb_similarities_wup = word_similarity(most_freq_verb, wn.VERB, metric='wup_similarity')\n",
    "    verb_similarities_path = word_similarity(most_freq_verb, wn.VERB, metric='path_similarity')\n",
    "    \n",
    "    # Sustantivo más frecuente\n",
    "    most_freq_noun = most_frequent_word(flat_tagged_text, 'NN')\n",
    "    noun_similarities_wup = word_similarity(most_freq_noun, wn.NOUN, metric='wup_similarity')\n",
    "    noun_similarities_path = word_similarity(most_freq_noun, wn.NOUN, metric='path_similarity')\n",
    "    \n",
    "    print(f\"Documento: {fileid}\")\n",
    "    print(f\"Verbo más frecuente: {most_freq_verb}\")\n",
    "    print(\"Similitudes WUP (verbo):\", verb_similarities_wup)\n",
    "    print(\"Similitudes Path (verbo):\", verb_similarities_path)\n",
    "    print(f\"Sustantivo más frecuente: {most_freq_noun}\")\n",
    "    print(\"Similitudes WUP (sustantivo):\", noun_similarities_wup)\n",
    "    print(\"Similitudes Path (sustantivo):\", noun_similarities_path)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Similitud de documentos con synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud entre Libro_1.txt y Libro_2.txt: 0.78\n",
      "Similitud entre Libro_1.txt y Libro_3.txt: 0.77\n",
      "Similitud entre Libro_1.txt y Libro_4.txt: 0.11\n",
      "Similitud entre Libro_1.txt y Libro_5.txt: 0.66\n",
      "Similitud entre Libro_1.txt y Libro_6.txt: 0.66\n",
      "Similitud entre Libro_1.txt y Libro_7.txt: 0.14\n",
      "Similitud entre Libro_1.txt y Libro_8.txt: 0.10\n",
      "Similitud entre Libro_1.txt y Libro_9.txt: 0.12\n",
      "Similitud entre Libro_1.txt y Libro_10.txt: 0.85\n",
      "Similitud entre Libro_2.txt y Libro_3.txt: 0.55\n",
      "Similitud entre Libro_2.txt y Libro_4.txt: 0.11\n",
      "Similitud entre Libro_2.txt y Libro_5.txt: 0.62\n",
      "Similitud entre Libro_2.txt y Libro_6.txt: 0.44\n",
      "Similitud entre Libro_2.txt y Libro_7.txt: 0.14\n",
      "Similitud entre Libro_2.txt y Libro_8.txt: 0.10\n",
      "Similitud entre Libro_2.txt y Libro_9.txt: 0.12\n",
      "Similitud entre Libro_2.txt y Libro_10.txt: 0.63\n",
      "Similitud entre Libro_3.txt y Libro_4.txt: 0.10\n",
      "Similitud entre Libro_3.txt y Libro_5.txt: 0.44\n",
      "Similitud entre Libro_3.txt y Libro_6.txt: 0.44\n",
      "Similitud entre Libro_3.txt y Libro_7.txt: 0.13\n",
      "Similitud entre Libro_3.txt y Libro_8.txt: 0.09\n",
      "Similitud entre Libro_3.txt y Libro_9.txt: 0.10\n",
      "Similitud entre Libro_3.txt y Libro_10.txt: 0.62\n",
      "Similitud entre Libro_4.txt y Libro_5.txt: 0.73\n",
      "Similitud entre Libro_4.txt y Libro_6.txt: 0.10\n",
      "Similitud entre Libro_4.txt y Libro_7.txt: 0.11\n",
      "Similitud entre Libro_4.txt y Libro_8.txt: 0.91\n",
      "Similitud entre Libro_4.txt y Libro_9.txt: 0.79\n",
      "Similitud entre Libro_4.txt y Libro_10.txt: 0.61\n",
      "Similitud entre Libro_5.txt y Libro_6.txt: 0.33\n",
      "Similitud entre Libro_5.txt y Libro_7.txt: 0.13\n",
      "Similitud entre Libro_5.txt y Libro_8.txt: 0.82\n",
      "Similitud entre Libro_5.txt y Libro_9.txt: 0.72\n",
      "Similitud entre Libro_5.txt y Libro_10.txt: 0.93\n",
      "Similitud entre Libro_6.txt y Libro_7.txt: 0.25\n",
      "Similitud entre Libro_6.txt y Libro_8.txt: 0.09\n",
      "Similitud entre Libro_6.txt y Libro_9.txt: 0.32\n",
      "Similitud entre Libro_6.txt y Libro_10.txt: 0.51\n",
      "Similitud entre Libro_7.txt y Libro_8.txt: 0.10\n",
      "Similitud entre Libro_7.txt y Libro_9.txt: 0.25\n",
      "Similitud entre Libro_7.txt y Libro_10.txt: 0.14\n",
      "Similitud entre Libro_8.txt y Libro_9.txt: 0.89\n",
      "Similitud entre Libro_8.txt y Libro_10.txt: 0.70\n",
      "Similitud entre Libro_9.txt y Libro_10.txt: 0.60\n"
     ]
    }
   ],
   "source": [
    "# Librerías utilizadas: nltk, wordnet\n",
    "# Funciones utilizadas: doc_to_synsets, document_path_similarity\n",
    "# Descripción: Extrae la frase más representativa de cada documento y calcula la similitud entre documentos usando synsets\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "# Extraer la frase más representativa y calcular similitud entre documentos\n",
    "frases_representativas = {}\n",
    "for fileid in corpus.fileids():\n",
    "    raw_text = corpus.raw(fileid)\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(cleaned_text)\n",
    "    primer_capitulo = ' '.join(sentences)\n",
    "    top_palabras = obtener_top_palabras(primer_capitulo, model, tokenizer)\n",
    "    frases_representativas[fileid] = ' '.join(top_palabras)\n",
    "\n",
    "# Comparar las frases más representativas utilizando la métrica \"Path_similarity\"\n",
    "comparacion_similitud = []\n",
    "libros = list(frases_representativas.keys())\n",
    "for i in range(len(libros)):\n",
    "    for j in range(i + 1, len(libros)):\n",
    "        doc1 = frases_representativas[libros[i]]\n",
    "        doc2 = frases_representativas[libros[j]]\n",
    "        similarity = document_path_similarity(doc1, doc2)\n",
    "        comparacion_similitud.append((libros[i], libros[j], similarity))\n",
    "\n",
    "# Mostrar los resultados\n",
    "for libro1, libro2, similarity in comparacion_similitud:\n",
    "    print(f\"Similitud entre {libro1} y {libro2}: {similarity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Similitud de palabras con embedding (GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento: Libro_1.txt\n",
      "Verbo más frecuente: said\n",
      "Términos más similares: ['said', 'told', 'says', 'spokesman', 'saying']\n",
      "\n",
      "\n",
      "Documento: Libro_2.txt\n",
      "Verbo más frecuente: said\n",
      "Términos más similares: ['said', 'told', 'says', 'spokesman', 'saying']\n",
      "\n",
      "\n",
      "Documento: Libro_3.txt\n",
      "Verbo más frecuente: came\n",
      "Términos más similares: ['came', 'took', 'after', 'saw', 'when']\n",
      "\n",
      "\n",
      "Documento: Libro_4.txt\n",
      "Verbo más frecuente: said\n",
      "Términos más similares: ['said', 'told', 'says', 'spokesman', 'saying']\n",
      "\n",
      "\n",
      "Documento: Libro_5.txt\n",
      "Verbo más frecuente: said\n",
      "Términos más similares: ['said', 'told', 'says', 'spokesman', 'saying']\n",
      "\n",
      "\n",
      "Documento: Libro_6.txt\n",
      "Verbo más frecuente: said\n",
      "Términos más similares: ['said', 'told', 'says', 'spokesman', 'saying']\n",
      "\n",
      "\n",
      "Documento: Libro_7.txt\n",
      "Verbo más frecuente: said\n",
      "Términos más similares: ['said', 'told', 'says', 'spokesman', 'saying']\n",
      "\n",
      "\n",
      "Documento: Libro_8.txt\n",
      "Verbo más frecuente: fleming\n",
      "Términos más similares: ['fleming', 'stuart', 'clarke', 'healy', 'fletcher']\n",
      "\n",
      "\n",
      "Documento: Libro_9.txt\n",
      "Verbo más frecuente: said\n",
      "Términos más similares: ['said', 'told', 'says', 'spokesman', 'saying']\n",
      "\n",
      "\n",
      "Documento: Libro_10.txt\n",
      "Verbo más frecuente: captain\n",
      "Términos más similares: ['captain', 'skipper', 'seaman', 'indies', 'cole']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Librerías utilizadas: numpy, scipy\n",
    "# Funciones utilizadas: load_glove_embeddings, find_closest_embeddings\n",
    "# Descripción: Encuentra las palabras más similares al verbo más frecuente usando embeddings de GloVe\n",
    "\n",
    "# Cargar las incrustaciones de GloVe\n",
    "embeddings_dict = load_glove_embeddings('glove.6B.50d.txt')\n",
    "\n",
    "# Encontrar los términos más similares al verbo más frecuente en cada documento\n",
    "for fileid, tagged_text in tagged_docs.items():\n",
    "    flat_tagged_text = [item for sublist in tagged_text for item in sublist]\n",
    "    \n",
    "    # Verbo más frecuente\n",
    "    most_freq_verb = most_frequent_word(flat_tagged_text, 'VB')\n",
    "    \n",
    "    # Embedding del verbo más frecuente\n",
    "    if most_freq_verb in embeddings_dict:\n",
    "        most_freq_verb_embedding = embeddings_dict[most_freq_verb]\n",
    "        similar_terms = find_closest_embeddings(most_freq_verb_embedding, embeddings_dict)\n",
    "        print(f\"Documento: {fileid}\")\n",
    "        print(f\"Verbo más frecuente: {most_freq_verb}\")\n",
    "        print(\"Términos más similares:\", similar_terms)\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Similitud de documentos con embedding (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud BERT entre Libro_1.txt y Libro_2.txt: 0.62\n",
      "Similitud BERT entre Libro_1.txt y Libro_3.txt: 0.65\n",
      "Similitud BERT entre Libro_1.txt y Libro_4.txt: 0.67\n",
      "Similitud BERT entre Libro_1.txt y Libro_5.txt: 0.65\n",
      "Similitud BERT entre Libro_1.txt y Libro_6.txt: 0.61\n",
      "Similitud BERT entre Libro_1.txt y Libro_7.txt: 0.68\n",
      "Similitud BERT entre Libro_1.txt y Libro_8.txt: 0.59\n",
      "Similitud BERT entre Libro_1.txt y Libro_9.txt: 0.69\n",
      "Similitud BERT entre Libro_1.txt y Libro_10.txt: 0.65\n",
      "Similitud BERT entre Libro_2.txt y Libro_3.txt: 0.89\n",
      "Similitud BERT entre Libro_2.txt y Libro_4.txt: 0.64\n",
      "Similitud BERT entre Libro_2.txt y Libro_5.txt: 0.67\n",
      "Similitud BERT entre Libro_2.txt y Libro_6.txt: 0.93\n",
      "Similitud BERT entre Libro_2.txt y Libro_7.txt: 0.63\n",
      "Similitud BERT entre Libro_2.txt y Libro_8.txt: 0.64\n",
      "Similitud BERT entre Libro_2.txt y Libro_9.txt: 0.69\n",
      "Similitud BERT entre Libro_2.txt y Libro_10.txt: 0.91\n",
      "Similitud BERT entre Libro_3.txt y Libro_4.txt: 0.66\n",
      "Similitud BERT entre Libro_3.txt y Libro_5.txt: 0.67\n",
      "Similitud BERT entre Libro_3.txt y Libro_6.txt: 0.92\n",
      "Similitud BERT entre Libro_3.txt y Libro_7.txt: 0.69\n",
      "Similitud BERT entre Libro_3.txt y Libro_8.txt: 0.61\n",
      "Similitud BERT entre Libro_3.txt y Libro_9.txt: 0.69\n",
      "Similitud BERT entre Libro_3.txt y Libro_10.txt: 0.87\n",
      "Similitud BERT entre Libro_4.txt y Libro_5.txt: 0.81\n",
      "Similitud BERT entre Libro_4.txt y Libro_6.txt: 0.65\n",
      "Similitud BERT entre Libro_4.txt y Libro_7.txt: 0.78\n",
      "Similitud BERT entre Libro_4.txt y Libro_8.txt: 0.89\n",
      "Similitud BERT entre Libro_4.txt y Libro_9.txt: 0.88\n",
      "Similitud BERT entre Libro_4.txt y Libro_10.txt: 0.69\n",
      "Similitud BERT entre Libro_5.txt y Libro_6.txt: 0.66\n",
      "Similitud BERT entre Libro_5.txt y Libro_7.txt: 0.75\n",
      "Similitud BERT entre Libro_5.txt y Libro_8.txt: 0.85\n",
      "Similitud BERT entre Libro_5.txt y Libro_9.txt: 0.88\n",
      "Similitud BERT entre Libro_5.txt y Libro_10.txt: 0.72\n",
      "Similitud BERT entre Libro_6.txt y Libro_7.txt: 0.64\n",
      "Similitud BERT entre Libro_6.txt y Libro_8.txt: 0.63\n",
      "Similitud BERT entre Libro_6.txt y Libro_9.txt: 0.70\n",
      "Similitud BERT entre Libro_6.txt y Libro_10.txt: 0.91\n",
      "Similitud BERT entre Libro_7.txt y Libro_8.txt: 0.72\n",
      "Similitud BERT entre Libro_7.txt y Libro_9.txt: 0.82\n",
      "Similitud BERT entre Libro_7.txt y Libro_10.txt: 0.66\n",
      "Similitud BERT entre Libro_8.txt y Libro_9.txt: 0.88\n",
      "Similitud BERT entre Libro_8.txt y Libro_10.txt: 0.68\n",
      "Similitud BERT entre Libro_9.txt y Libro_10.txt: 0.72\n"
     ]
    }
   ],
   "source": [
    "# Librerías utilizadas: torch, transformers\n",
    "# Funciones utilizadas: bert_similarity\n",
    "# Descripción: Calcula la similitud entre documentos usando embeddings de BERT\n",
    "\n",
    "# Cargar el modelo BERT pre-entrenado y el tokenizador (largo y uncased)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "# Comparar las frases más representativas utilizando BERT\n",
    "comparacion_similitud_bert = []\n",
    "for i in range(len(libros)):\n",
    "    for j in range(i + 1, len(libros)):\n",
    "        doc1 = frases_representativas[libros[i]]\n",
    "        doc2 = frases_representativas[libros[j]]\n",
    "        similarity = bert_similarity(doc1, doc2, bert_model, bert_tokenizer)\n",
    "        comparacion_similitud_bert.append((libros[i], libros[j], similarity))\n",
    "\n",
    "# Mostrar los resultados de BERT\n",
    "for libro1, libro2, similarity in comparacion_similitud_bert:\n",
    "    print(f\"Similitud BERT entre {libro1} y {libro2}: {similarity:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
