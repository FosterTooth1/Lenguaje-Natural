{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROCESAMIENTO DE LENGUAJE NATURAL\n",
    "\n",
    "PRACTICA 2\n",
    "\n",
    "FLORES LARA ALBERTO\n",
    "\n",
    "5BV1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARTE 2. VECTORIZACIÓN DE DOCUMENTOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Obtener los documentos resultado de una etapa de normalización. En específico, después de:\n",
    "a. Convertir cada palabra a minúscula y de remover las “stop-words” y signos de puntuación.\n",
    "b. Aplicar la técnica de “stemming”.\n",
    "c. Aplicar POS-Tagging\n",
    "d. Aplicar “lemmatization”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos las librerias que vamos a utilizar provenientes de nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Convertir cada palabra a minúscula y de remover las “stop-words” y signos de puntuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos todas las funciones que vamos a ocupar\n",
    "# Definimos una funcion para obtener los documentos previamente almacenados en archivos txt \n",
    "def obtenertxt(documento):\n",
    "    with open(documento, 'r', encoding='utf-8') as archivo:\n",
    "        Texto=archivo.read()\n",
    "    return Texto\n",
    "\n",
    "# Definimos una funcion para convertir cada palabra a minusculas\n",
    "def minusculas(texto):\n",
    "    Texto_minusculas=texto.lower()\n",
    "    return Texto_minusculas\n",
    "\n",
    "# Definimos una funcion para quitar todos los caracteres especiales\n",
    "def caracteresesp(texto):\n",
    "    Texto_sin_caracteres=re.sub(r'[^a-zA-Z0-9\\s]', '', texto)\n",
    "    return Texto_sin_caracteres\n",
    "\n",
    "# Definimos una funcion para quitar todas las stopwords\n",
    "def stooopwords(tokens):\n",
    "    # Se inicializa el conjunto de stopwords en inglés.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Se crea una lista para almacenar los tokens sin stopwords.\n",
    "    texto_sin_stopword = []\n",
    "\n",
    "    # Se almacenan los tokens que no son stopwords.\n",
    "    for w in tokens:\n",
    "        if w not in stop_words:\n",
    "            texto_sin_stopword.append(w)\n",
    "            \n",
    "    return texto_sin_stopword\n",
    "\n",
    "# Definimos una funcion para tokenizar los documentos\n",
    "def tokenizar(texto):\n",
    "    tokens = word_tokenize(texto)\n",
    "    return tokens\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
