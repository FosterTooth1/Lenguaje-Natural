{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROCESAMIENTO DE LENGUAJE NATURAL\n",
    "\n",
    "PRACTICA 2\n",
    "\n",
    "FLORES LARA ALBERTO\n",
    "\n",
    "5BV1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARTE 2. VECTORIZACIÓN DE DOCUMENTOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Obtener los documentos resultado de una etapa de normalización. En específico, después de:\n",
    "a. Convertir cada palabra a minúscula y de remover las “stop-words” y signos de puntuación.\n",
    "b. Aplicar la técnica de “stemming”.\n",
    "c. Aplicar POS-Tagging\n",
    "d. Aplicar “lemmatization”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos las librerias que vamos a utilizar provenientes de nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Convertir cada palabra a minúscula y de remover las “stop-words” y signos de puntuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos todas las funciones que vamos a ocupar\n",
    "# Definimos una funcion para obtener los documentos previamente almacenados en archivos txt \n",
    "def obtenertxt(documento):\n",
    "    with open(documento, 'r', encoding='utf-8') as archivo:\n",
    "        Texto=archivo.read()\n",
    "    return Texto\n",
    "\n",
    "# Definimos una funcion para convertir cada palabra a minusculas\n",
    "def minusculas(texto):\n",
    "    Texto_minusculas=texto.lower()\n",
    "    return Texto_minusculas\n",
    "\n",
    "# Definimos una funcion para quitar todos los caracteres especiales\n",
    "def caracteresesp(texto):\n",
    "    Texto_sin_caracteres=re.sub(r'[^a-zA-Z0-9\\s]', '', texto)\n",
    "    return Texto_sin_caracteres\n",
    "\n",
    "# Definimos una funcion para quitar todas las stopwords\n",
    "def stooopwords(tokens):\n",
    "    # Se inicializa el conjunto de stopwords en inglés.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Se crea una lista para almacenar los tokens sin stopwords.\n",
    "    texto_sin_stopword = []\n",
    "\n",
    "    # Se almacenan los tokens que no son stopwords.\n",
    "    for w in tokens:\n",
    "        if w not in stop_words:\n",
    "            texto_sin_stopword.append(w)\n",
    "            \n",
    "    return texto_sin_stopword\n",
    "\n",
    "# Definimos una funcion para tokenizar los documentos\n",
    "def tokenizar(texto):\n",
    "    tokens = word_tokenize(texto)\n",
    "    return tokens\n",
    "\n",
    "doc1=obtenertxt('Doc1.txt')\n",
    "doc2=obtenertxt('Doc2.txt')\n",
    "doc3=obtenertxt('Doc3.txt')\n",
    "\n",
    "doc1=caracteresesp(doc1)\n",
    "doc2=caracteresesp(doc2)\n",
    "doc3=caracteresesp(doc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos son los documentos originales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pancreatic cancer with metastasis Jaundice with transaminitis evaluate for obstruction process\n",
      "Pancreatitis Breast cancer No output from enteric tube Assess tube\n",
      "Metastasis pancreatic cancer Acute renal failure evaluate for hydronephrosis or obstructive uropathy\n"
     ]
    }
   ],
   "source": [
    "print(doc1)\n",
    "print(doc2)\n",
    "print(doc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos son los documentos con minusculas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pancreatic cancer with metastasis jaundice with transaminitis evaluate for obstruction process\n",
      "pancreatitis breast cancer no output from enteric tube assess tube\n",
      "metastasis pancreatic cancer acute renal failure evaluate for hydronephrosis or obstructive uropathy\n"
     ]
    }
   ],
   "source": [
    "doc1=minusculas(doc1)\n",
    "doc2=minusculas(doc2)\n",
    "doc3=minusculas(doc3)\n",
    "\n",
    "print(doc1)\n",
    "print(doc2)\n",
    "print(doc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos son los documentos sin caracteres especiales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pancreatic cancer with metastasis jaundice with transaminitis evaluate for obstruction process\n",
      "pancreatitis breast cancer no output from enteric tube assess tube\n",
      "metastasis pancreatic cancer acute renal failure evaluate for hydronephrosis or obstructive uropathy\n"
     ]
    }
   ],
   "source": [
    "doc1=caracteresesp(doc1)\n",
    "doc2=caracteresesp(doc2)\n",
    "doc3=caracteresesp(doc3)\n",
    "\n",
    "print(doc1)\n",
    "print(doc2)\n",
    "print(doc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos son los tokens de cada documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pancreatic', 'cancer', 'with', 'metastasis', 'jaundice', 'with', 'transaminitis', 'evaluate', 'for', 'obstruction', 'process']\n",
      "['pancreatitis', 'breast', 'cancer', 'no', 'output', 'from', 'enteric', 'tube', 'assess', 'tube']\n",
      "['metastasis', 'pancreatic', 'cancer', 'acute', 'renal', 'failure', 'evaluate', 'for', 'hydronephrosis', 'or', 'obstructive', 'uropathy']\n"
     ]
    }
   ],
   "source": [
    "tokens1=tokenizar(doc1)\n",
    "tokens2=tokenizar(doc2)\n",
    "tokens3=tokenizar(doc3)\n",
    "\n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "print(tokens3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos son los tokens sin stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pancreatic', 'cancer', 'metastasis', 'jaundice', 'transaminitis', 'evaluate', 'obstruction', 'process']\n",
      "['pancreatitis', 'breast', 'cancer', 'output', 'enteric', 'tube', 'assess', 'tube']\n",
      "['metastasis', 'pancreatic', 'cancer', 'acute', 'renal', 'failure', 'evaluate', 'hydronephrosis', 'obstructive', 'uropathy']\n"
     ]
    }
   ],
   "source": [
    "tokens1=stooopwords(tokens1)\n",
    "tokens2=stooopwords(tokens2)\n",
    "tokens3=stooopwords(tokens3)\n",
    "\n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "print(tokens3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Aplicar la técnica de “stemming”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pancreat', 'cancer', 'metastasi', 'jaundic', 'transamin', 'evalu', 'obstruct', 'process']\n",
      "['pancreat', 'breast', 'cancer', 'output', 'enter', 'tube', 'assess', 'tube']\n",
      "['metastasi', 'pancreat', 'cancer', 'acut', 'renal', 'failur', 'evalu', 'hydronephrosi', 'obstruct', 'uropathi']\n"
     ]
    }
   ],
   "source": [
    "# Se nicializa la funcion para realizar stemming.\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stemming(tokens):\n",
    "    # Se crea una lista para almacenar los tokens con stemming.\n",
    "    stemming_list=[]\n",
    "    # Se almacenan los tokens con stemming aplicado.\n",
    "    for words in tokens:\n",
    "        stemming_list.append(ps.stem(words))\n",
    "    return stemming_list\n",
    "\n",
    "tokens1=stemming(tokens1)\n",
    "tokens2=stemming(tokens2)\n",
    "tokens3=stemming(tokens3)\n",
    "\n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "print(tokens3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Aplicar POS-Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pancreat', 'NN'), ('cancer', 'NN'), ('metastasi', 'NN'), ('jaundic', 'JJ'), ('transamin', 'NN'), ('evalu', 'JJ'), ('obstruct', 'NN'), ('process', 'NN')]\n",
      "[('pancreat', 'NN'), ('breast', 'NN'), ('cancer', 'NN'), ('output', 'NN'), ('enter', 'VBP'), ('tube', 'NN'), ('assess', 'NN'), ('tube', 'NN')]\n",
      "[('metastasi', 'NN'), ('pancreat', 'NN'), ('cancer', 'NN'), ('acut', 'VBD'), ('renal', 'JJ'), ('failur', 'NN'), ('evalu', 'NN'), ('hydronephrosi', 'NN'), ('obstruct', 'NN'), ('uropathi', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "def pos_tagging(tokens):\n",
    "    # Realizar el etiquetado\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    return tagged_tokens\n",
    "\n",
    "tokens1_Pt=pos_tagging(tokens1)\n",
    "tokens2_Pt=pos_tagging(tokens2)\n",
    "tokens3_Pt=pos_tagging(tokens3)\n",
    "\n",
    "print(tokens1_Pt)\n",
    "print(tokens2_Pt)\n",
    "print(tokens3_Pt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Aplicar Lematizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pancreat', 'cancer', 'metastasi', 'jaundic', 'transamin', 'evalu', 'obstruct', 'process']\n",
      "['pancreat', 'breast', 'cancer', 'output', 'enter', 'tube', 'ass', 'tube']\n",
      "['metastasi', 'pancreat', 'cancer', 'acut', 'renal', 'failur', 'evalu', 'hydronephrosi', 'obstruct', 'uropathi']\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos la etiqueta Pos de WordNet a partir de la etiqueta del Poustagging\n",
    "def obtener_pos_wordnet(etiqueta_treebank):\n",
    "    if etiqueta_treebank.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif etiqueta_treebank.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif etiqueta_treebank.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif etiqueta_treebank.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Función para lematizar una lista de tokens etiquetados\n",
    "def lematizar_tokens_Post(tokens_etiquetados):\n",
    "    lematizador = WordNetLemmatizer()  # Inicializa el lematizador\n",
    "    tokens_lematizados = []  # Lista para almacenar los tokens lematizados\n",
    "    # Itera sobre cada token y su etiqueta\n",
    "    for palabra, etiqueta in tokens_etiquetados:\n",
    "        etiqueta_wn = obtener_pos_wordnet(etiqueta)  # Obtiene la etiqueta de WordNet\n",
    "        # lematiza con base en la etiqueta correspondiente para mayor precisión\n",
    "        tokens_lematizados.append(lematizador.lemmatize(palabra, pos=etiqueta_wn))\n",
    "    return tokens_lematizados\n",
    "\n",
    "tokens1=lematizar_tokens_Post(tokens1_Pt)\n",
    "tokens2=lematizar_tokens_Post(tokens2_Pt)\n",
    "tokens3=lematizar_tokens_Post(tokens3_Pt)\n",
    "\n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "print(tokens3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
